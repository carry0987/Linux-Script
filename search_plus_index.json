[{"url":"./","title":"Introduction","level":"1.1","keywords":[],"body":"Linux-Script Store Linux Script ‚ö†Ô∏è This repository has been archived and is no longer actively maintained. It has been moved to the Linux-Note. Please refer to the new repository for the latest updates and contributions. Info Please run the script with root user, to change to root user, use the below command su root Contact Website Link Github @carry0987 "},{"url":"Common/","title":"Regular Command","level":"1.2","keywords":[],"body":"Common Setting .bashrc alias ll='ls -lh' alias la='ls -a' alias tt='sudo sh tools' alias sr='screen -r' Hint If bash: command not found export PATH=\"$PATH:/sbin:/usr/sbin:usr/local/sbin\" Add ssh-key for SSH connection sudo vim ~/.ssh/authorized_keys "},{"url":"Common/CommandList.html","title":"Command List","level":"1.2.1","keywords":[],"body":"Linux Command List Count folder's size du -sch /path/to/folder/*type Set Time & Date NTP sudo timedatectl set-ntp yes Set default shell sudo ln -fs /bin/bash /bin/sh Make zstdmt as zstd zstdmt is faster than zstd, since it uses multi-threading. sudo ln -s /usr/bin/zstdmt /usr/bin/zstd Add user & group sudo adduser [username] P.S. Don't use useradd to add user, it will not create home directory for user. Add user to sudo group sudo usermod -aG sudo [username] Change user password sudo passwd [username] Delete user sudo userdel -r -f [username] Delete group sudo groupdel -f [groupname] Change default editor to vim sudo update-alternatives --config editor And then choose vim.basic: There are 3 choices for the alternative editor (providing /usr/bin/editor). Selection Path Priority Status ------------------------------------------------------------ * 0 /bin/nano 40 auto mode 1 /bin/nano 40 manual mode 2 /usr/bin/vim.basic 30 manual mode 3 /usr/bin/vim.tiny 15 manual mode Press <enter> to keep the current choice[*], or type selection number: 2 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in manual mode Use tee instead of echo to add lines into file The redirection is done by the shell before sudo is even started.So either make sure the redirection happens in a shell with the right permissions sudo bash -c 'echo \"hello\" > f.txt' Or use tee echo \"hello\" | sudo tee f.txt # add -a for append (>>) Scan WiFi sudo iwlist wlan0 scan Search Depend packages of Application Way 1 apt-cache depends build-essential Way 2 apt-cache showpkg build-essential APT Error Fix If it shows E: Encountered a section with no Package: header Just clean the error link sudo rm /var/lib/apt/lists/* -vf And then sudo apt-get update Start SSH sudo /etc/init.d/ssh start Change SSH Port sudo vim /etc/ssh/sshd_config then sudo service ssh restart Regenerate SSH Key ssh-keygen -R \"you server hostname or ip\" Set default path of terminal echo 'cd ~/Desktop/' >> ~/.bashrc If in MacOS, use echo 'cd ~/Desktop/' >> ~/.zprofile Clear history history -c If in MacOS, and you're using zsh as default shell, use history -p Zip zip -h Show the help interface -m After the file is compressed, delete the original file -o Set the latest change time of all files in the archive to the time of compression -q Quiet mode, does not display the execution of instructions during compression -r Treats all subdirectories under the specified directory together with the file -P Set the compression password Unzip unzip -l Show the files whitch contained in the compressed file -t Check if the compressed file is correct -P<password> Unzip files with password Terminal Ctrl + Alt + F2 = Terminal Alt + F7 = Exit Terminal Rclone Show process every 1 second when upload files: rclone copy -v --stats 1s [File] [Target] Remove rclone: sudo rm /usr/bin/rclone sudo rm /usr/local/share/man/man1/rclone.1 Rclone Config File rclone -h | grep \"Config file\" Screen Kill specific screen screen -S [pid/name] -X quit Or kill [pid] Kill all screen killall screen "},{"url":"Common/Sudo.html","title":"Run SUDO without password","level":"1.2.2","keywords":[],"body":"Enable sudo without password in Ubuntu/Debian You probably know that in Ubuntu/Debian, you should not run as the root user, butshould use the sudo command instead to run commands that require rootprivileges. However, it can also be inconvenient to have to enter your passwordevery time that you use sudo. Here‚Äôs a quick fix that removes the requirement toenter you password for sudo. Step Open the /etc/sudoers file (as root, of course!) by running: sudo visudo You should never edit /etc/sudoers with a regular text editor, such as Vim or nano,because they do not validate the syntax like the visudo editor. At the end of the /etc/sudoers file add this line: username ALL=(ALL) NOPASSWD:ALL Replace username with your account username, of course. Save the file andexit with <ESC>wq. If you have any sort of syntax problem, visudo willwarn you and you can abort the change or open the file for editing again. It is important to add this line at the end of the file,so that the other permissions do not override this directive, since they are processed in order. Finally, open a new terminal window and run a command that requires root privileges,such as sudo apt update. You should not be prompted for your password! That‚Äôs it! Enjoy your new freedom! üôÇ Add user to sudoer sudo usermod -aG sudo [username] "},{"url":"Common/ChangeUsername.html","title":"Change Username","level":"1.2.3","keywords":[],"body":"Change username In the Linux operating system, usernames are essential for identifying and managing user accounts. A username is a unique identity for a user that allows them to log in, access files, and perform certain tasks on the system. However, there may be times when you need to change your username owing to personal preferences or security concerns. Fortunately, Linux provides various options for changing usernames, giving flexibility and simplicity to both system administrators and users. usermod The usermod command is a powerful utility that allows system administrators to change the username and other attributes of a user account.You can modify a username with the usermod command by following a few simple steps. Check Existing Username Before making any changes, it is essential to verify the current username of the account you wish to modify.This step ensures that you have accurate information and helps avoid any confusion during the renaming process.To check the current username, use the following command in the terminal: whoami Change the Username Once you have identified the account's current username, you can use the usermod command to change it.The usermod command allows you to modify various attributes of a user account, including the username. The syntax for the usermod command to change the username is as follows: sudo usermod -l new_username old_username Replace \"new_username\" with the desired new username and \"old_username\" with the current username of the account.The 'sudo' command is used to execute the usermod command with administrative privileges. Update the User's Home Directory and Group After changing the username, it is crucial to update the user's home directory and group to reflect the new username. This step ensures consistency and prevents any conflicts or inconsistencies in the system. To update the user's home directory and group, use the following command: sudo usermod -d /home/new_username -m new_username In this command, replace \"new_username\" with the newly assigned username.The '-d' option specifies the new home directory path and '-m' ensures the user's files are moved to the new directory. Verify the Changes To confirm that the username has been successfully changed, you can use the following command: whoami The terminal should display the new username associated with the current session, indicating that the changes have taken effect. "},{"url":"Common/ChangeNetworkInterfaceName.html","title":"Change Network Interface Name","level":"1.2.4","keywords":[],"body":"Change default network interface name on Ubuntu If you ever interested in changing interface names to old type ethX, here is the tutorial for you. As you can see in the following command, my system is having a network adapter called ens33. This is just the case of VMware environment, it may vary depends on the hardware but the steps to get back ethX will be the same. $ ip a 1: lo: <loopback,up,lower_up> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: </loopback,up,lower_up>ens33: <broadcast,multicast,up,lower_up> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:05:a3:e2 brd ff:ff:ff:ff:ff:ff </broadcast,multicast,up,lower_up>inet 192.168.12.12/24 brd 192.168.12.255 scope global dynamic ens33 valid_lft 1683sec preferred_lft 1683sec inet6 fe80::20c:29ff:fe05:a3e2/64 scope link valid_lft forever preferred_lft forever From the dmesg command, you can see that the device got renamed during the system boot. $ dmesg | grep -i eth [ 3.050064] e1000 0000:02:01.0 eth0: (PCI:66MHz:32-bit) 00:0c:29:05:a3:e2 [ 3.050074] e1000 0000:02:01.0 eth0: Intel(R) PRO/1000 Network Connection [ 3.057410] e1000 0000:02:01.0 ens33: renamed from eth0 To get an ethX back, edit the grub file. sudo vim /etc/default/grub Look for GRUB_CMDLINE_LINUX and add the following net.ifnames=0 biosdevname=0. From: GRUB_CMDLINE_LINUX=\"\" To: GRUB_CMDLINE_LINUX=\"net.ifnames=0 biosdevname=0\" Generate a new grub file using the following command. $ sudo grub-mkconfig -o /boot/grub/grub.cfg Generating grub configuration file ... Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. Found linux image: /boot/vmlinuz-4.4.0-15-generic Found initrd image: /boot/initrd.img-4.4.0-15-generic Found memtest86+ image: /memtest86+.elf Found memtest86+ image: /memtest86+.bin done Reboot your system. sudo reboot After the system reboot, just check whether you have an ethX back. $ ip a 1: lo: <loopback,up,lower_up> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: </loopback,up,lower_up>eth0: <broadcast,multicast,up,lower_up> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:05:a3:e2 brd ff:ff:ff:ff:ff:ff </broadcast,multicast,up,lower_up>inet 192.168.12.12/24 brd 192.168.12.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe05:a3e2/64 scope link valid_lft forever preferred_lft forever "},{"url":"Common/DisableCloudInit.html","title":"Disable cloud-init","level":"1.2.5","keywords":[],"body":"Disable cloud-init on Ubuntu This step-by-step tutorial will guide you on how to remove cloud-init and unattended-upgrades from your Ubuntu system. cloud-init is the Ubuntu package that handles early initialization of a cloud instance. unattended-upgrades allows installation of security upgrades automatically. If you no longer need cloud-init and unattended-upgrades, here is how to remove them and ensure your instance is in a desired state. Step 1: Remove cloud-init First, we need to purg cloud-init. Purging will not only remove the package but also the config files. Enter the following command in your terminal: sudo apt purge cloud-init Step 2: Remove cloud-init Configuration Files Now, let's remove the remaining cloud-init configuration files to prevent the system from referencing them in the future. sudo rm -rvf /etc/cloud Step 3: Remove unattended-upgrades To remove unattended-upgrades, input the following command: sudo apt purge unattended-upgrades Step 4: Remove unattended-upgrades Logs Next, we want to remove the log files generated by unattended-upgrades: sudo rm -rvf /var/log/unattended-upgrades Step 5: Remove Unnecessary Packages Now, we will use autoremove option to remove packages that were automatically installed to satisfy dependencies for other packages and now are no longer needed: sudo apt autoremove Step 6: Install netplan.io Running the following command will install netplan.io, a network configuration utility: sudo apt install netplan.io Step 7: Install isc-dhcp-client Finally, we'll need to install isc-dhcp-client, which is a DHCP client used to automatically obtain and update IP addresses: sudo apt install isc-dhcp-client Now, you have successfully removed cloud-init and unattended-upgrades from your Ubuntu system. Please feel free to revisit this guide whenever you need assistance. "},{"url":"Crontab/","title":"Crontab","level":"1.3","keywords":[],"body":"Crontab Setting up cron jobs in Unix, Solaris & Linux Quick Reference cron is a Unix, solaris, Linux utility that allows tasks to be automatically run in the background at regular intervals by the cron daemon. cron meaning ‚Äì There is no definitive explanation but most accepted answers is reportedly from Ken Thompson ( author of unix cron ), name cron comes from chron ,the Greek prefix for time What is cron ? ‚Äì Cron is a daemon which runs at the times of system boot from /etc/init.d scripts. If needed it can be stopped/started/restart using init script or with command service crond start in Linux systems. This document covers following aspects of Unix, Linux cron jobs to help you understand and implement cronjobs successfully "},{"url":"Crontab/what_is_cron.html","title":"What is cron","level":"1.3.1","keywords":[],"body":"What is crontab? Crontab (CRON TABle) is a file which contains the schedule of cron entries to be run and at specified times. File location varies by operating systems, See Crontab file location at the end of this document. What is a cron job or cron schedule? Cron job or cron schedule is a specific set of execution instructions specifing day, time and command to execute. crontab can have multiple execution statments. "},{"url":"Crontab/restrictions.html","title":"Crontab Restrictions","level":"1.3.2","keywords":[],"body":"Crontab Restrictions You can execute crontab if your name appears in the file /usr/lib/cron/cron.allow. If that file does not exist, you can use crontab if your name does not appear in the file /usr/lib/cron/cron.deny. If only cron.deny exists and is empty, all users can use crontab. If neither file exists, only the root user can use crontab. The allow/deny files consist of one user name per line. "},{"url":"Crontab/commands.html","title":"Crontab Commands","level":"1.3.3","keywords":[],"body":"Crontab Commands To specify a editor to open crontab file, use: export EDITOR=vi ; Below is a list of crontab commands: Command Description crontab -e Edit crontab file, or create one if it doesn‚Äôt already exist. crontab -l Display crontab list of cron jobs, display crontab file contents. crontab -r Remove your crontab file. crontab -v Display the last time you edited your crontab file. (This option is only available on a few systems.) Service Commands Below is a list of crontab service commands: Command Description sudo service cron status Check the status of cron service. sudo service cron start Start the cron service. sudo service cron stop Stop the cron service. sudo service cron reload Reload the cron service configuration. sudo service cron restart Restart the cron service. "},{"url":"Crontab/syntax.html","title":"Crontab Syntax","level":"1.3.4","keywords":[],"body":"Crontab Syntax A crontab file has five fields for specifying day , date and time followed by the command to be run at that interval. * in the value field above means all legal values as in braces for that column. The value column can have a * or a list of elements separated by commas. An element is either a number in the ranges shown above or two numbers in the range separated by a hyphen (meaning an inclusive range). Notes: Repeat pattern like /2 for every 2 minutes or /10 for every 10 minutes is not supported by all operating systems. If you try to use it and crontab complains it is probably not supported. The specification of days can be made in two fields: month day and weekday. If both are specified in an entry, they are cumulative meaning both of the entries will get executed. "},{"url":"Crontab/example.html","title":"Crontab Example","level":"1.3.5","keywords":[],"body":"Crontab Examples A line in crontab file like below removes the tmp files from /home/someuser/tmp each day at 6:30 PM. 30 18 * * * rm /home/someuser/tmp/* cron every hour This is most commonly used for running cron every hour and executing a command after an interval of one hour. crontab format every hour is simple to have hour field as * which runs every hour as the clock switches to new hour. If you want to run it at the beginning of hour the minute filed needs to be 0 or any other minutes when you want to run it at a specific minute of the hour. cron every hour to run at the beginning of the hour 00 * * * * rm /home/someuser/tmp/* cron every hour to run at 15 minute of an hour 15 * * * * rm /home/someuser/tmp/* cron every minute To run cron every minute keep the minutes field as *, as minute changes to new minute cron will be executed every minute. If you want to run it continuously every hour then the hour field also needs to have value of *. * * * * * rm /home/someuser/tmp/* If you want to run a script every minute at specific hour, change the value of hour field to specific value such as 11th hour. * 11 * * * rm /home/someuser/tmp/* Changing the parameter values as below will cause this command to run at different time schedule below: min hour day/month month day/week Execution time 30 0 1 1,6,12 * 00:30 Hrs on 1st of Jan, June & Dec. 0 20 * 10 1-5 8.00 PM every weekday (Mon-Fri) only in Oct. 0 0 1,10,15 * * midnight on 1st,10th & 15th of month 5,10 0 10 * 1 At 12.05,12.10 every Monday & on 10th of every month Note: If you inadvertently enter the crontab command with no argument(s), do not attempt to get out with Control-d. This removes all entries in your crontab file. Instead, exit with Control-c. Run cron job in specific time If you want to set crontab in specific time, for instance, running at midnight, 0 0 is the correct specification for midnight (no leading zeros, so in this case no double zero). From man crontab(5) field allowed values ----- -------------- minute 0-59 hour 0-23 day of month 1-31 month 1-12 (or names, see below) day of week 0-7 (0 or 7 is Sun, or use names) "},{"url":"Crontab/environment.html","title":"Crontab Environment","level":"1.3.6","keywords":[],"body":"Crontab Environment cron invokes the command from the user's HOME directory with the shell, /usr/bin/sh.cron supplies a default environment for every shell, defining: HOME=[user-home-directory] LOGNAME=[user-login-id] PATH=/usr/bin:/usr/sbin:. SHELL=/usr/bin/sh Users who desire to have their .profile executed must explicitly do so in the crontab entry or in a script called by the entry. "},{"url":"Crontab/log_file_and_disable_email.html","title":"Log file & Disable Email","level":"1.3.7","keywords":[],"body":"Disable Email By default, cron jobs send an email to the user account executing the cronjob. This can inundate your inbox with messages regarding your cronjobs. If this is not desired, you can disable these emails. To suppress the email alerts, append the following command at the end of the cron job line. >/dev/null 2>&1 This command redirects both standard output (file descriptor 1) and standard error (file descriptor 2) to /dev/null, which is a special file that discards all data written to it (but reports that the write operation succeeded). In simple words, this operation is discarding both the regular output and error messages, hence, no email is sent. Generate log file To collect the cron execution execution log in a file: 30 18 * * * rm /home/someuser/tmp/* > /home/someuser/cronlogs/clean_tmp_dir.log "},{"url":"Crontab/file_location.html","title":"Crontab file location","level":"1.3.8","keywords":[],"body":"Crontab file location User crontab files are stored by the login names in different locations in different Unix and Linux flavors. These files are useful for backing up, viewing and restoring but should be edited only with crontab command by the users. Operating System Location MacOS /usr/lib/cron/tabs/ BSD Unix /var/cron/tabs/ Solaris, HP-UX, Debian, Ubuntu /var/spool/cron/crontabs/ AIX, Red Hat Linux, CentOS, Ferdora /var/spool/cron/ "},{"url":"Tools/","title":"Tools","level":"1.4","keywords":[],"body":"Tools tools.sh Description: Regular Command Support OS: Debian 6+ / Ubuntu 14+ Installation: WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Tools/tools.sh && chmod +x tools.sh && mv -v tools.sh /usr/local/bin/tools && bash tools CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Tools/tools.sh && chmod +x tools.sh && mv -v tools.sh /usr/local/bin/tools && bash tools "},{"url":"Setup/","title":"Setup","level":"1.5","keywords":[],"body":"Setup Set-LC.sh Description: Run this script to setup default environment Support OS: Debian 6+ / Ubuntu 14+ Installation: WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Setup/Set-LC.sh && chmod +x Set-LC.sh && bash Set-LC.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Setup/Set-LC.sh && chmod +x Set-LC.sh && bash Set-LC.sh Install-Main-Pkg.sh Description: Install main packages for new server Support OS: Debian 6+ / Ubuntu 14+ Installation: WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Setup/Install-Main-Pkg.sh && chmod +x Install-Main-Pkg.sh && bash Install-Main-Pkg.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Setup/Install-Main-Pkg.sh && chmod +x Install-Main-Pkg.sh && bash Install-Main-Pkg.sh "},{"url":"Vim/","title":"Vim","level":"1.6","keywords":[],"body":"Vim vim-setup.sh Description: Run this script to setup vim config Installation: WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Vim/vim-setup.sh && chmod +x vim-setup.sh && bash vim-setup.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Vim/vim-setup.sh && chmod +x vim-setup.sh && bash vim-setup.sh .vimrc set nu set ai set shiftwidth=4 set tabstop=4 set expandtab set mouse=a set cursorline set hlsearch set viminfo+=n~/.vim/viminfo set t_Co=256 set encoding=utf8 syntax on Paste content Shift + Ins Or Shift + right-mouse-click "},{"url":"Htop/","title":"Htop","level":"1.7","keywords":[],"body":"Htop Htop is a terminal-based system monitor. It is similar to top, but allows you to scroll vertically and horizontally, so you can see all the processes running on the system, along with their full command lines. Installation: WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Htop/htop-setup.sh && chmod +x htop-setup.sh && bash htop-setup.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Htop/htop-setup.sh && chmod +x htop-setup.sh && bash htop-setup.sh htoprc htop can be configured by editing ~/.config/htop/htoprc. The following is my htoprc file: # Beware! This file is rewritten by htop when settings are changed in the interface. # The parser is also very primitive, and not human-friendly. fields=0 48 17 18 38 39 40 2 46 47 49 1 sort_key=46 sort_direction=-1 tree_sort_key=0 tree_sort_direction=1 hide_kernel_threads=1 hide_userland_threads=0 shadow_other_users=0 show_thread_names=0 show_program_path=1 highlight_base_name=0 highlight_megabytes=1 highlight_threads=1 highlight_changes=0 highlight_changes_delay_secs=5 find_comm_in_cmdline=1 strip_exe_from_cmdline=1 show_merged_command=0 tree_view=0 tree_view_always_by_pid=0 header_margin=1 detailed_cpu_time=0 cpu_count_from_one=0 show_cpu_usage=1 show_cpu_frequency=0 show_cpu_temperature=0 degree_fahrenheit=0 update_process_names=0 account_guest_in_cpu_meter=0 color_scheme=0 enable_mouse=1 delay=15 left_meters=Hostname NetworkIO DiskIO CPU Memory Swap left_meter_modes=2 2 2 1 1 1 right_meters=DateTime Uptime Tasks LoadAverage Systemd right_meter_modes=2 2 2 2 2 hide_function_bar=0 "},{"url":"Docker/setup.html","title":"Docker Setup","level":"1.8.1","keywords":[],"body":"Docker Install Follow Docker Installation Guide via this LINK Setup Start docker-client and set automatically start on boot Docker sudo systemctl start docker && sudo systemctl enable docker Containerd sudo systemctl start containerd && sudo systemctl enable containerd To stop this behavior, use disable instead. sudo systemctl disable docker sudo systemctl disable containerd Run Docker Without Sudo You can set up the Docker daemon to run without root privileges. This is a good way to avoid using sudo when you use the docker command.Directly use the following script for a quick setup, or follow the steps below for manual operation. Quick Setup WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Docker/docker-setup.sh && chmod +x docker-setup.sh && bash docker-setup.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Docker/docker-setup.sh && chmod +x docker-setup.sh && bash docker-setup.sh Manual Setup Add docker group sudo groupadd docker Add current user into docker group sudo usermod -aG docker $USER Refresh group list newgrp docker Check group groups $USER Restart docker service sudo systemctl restart docker Test Docker without sudo docker info Docker-compose (Optional) Download the corresponding Linux version of Docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/v2.24.6/docker-compose-$(uname -s)-$(uname -m)\" \\ -o /usr/local/bin/docker-compose Give execute permission sudo chmod +x /usr/local/bin/docker-compose "},{"url":"Docker/command.html","title":"Docker Command","level":"1.8.2","keywords":[],"body":"Docker Command Build docker image To build your Dockerfile into an Image and upload it to DockerHub, you can follow the steps below: Create a Docker Image: In the directory where your Dockerfile is located, execute the following command to create a Docker image: docker build -t dockerhub_username/image_name:your_tag . dockerhub_username: Your username on DockerHub. image_name: The name you want to give to your image. your_tag: Tag version, e.g., latest or v1.0 etc. Example: docker build -t myusername/myphpimage:latest . Log into DockerHub: If you're not yet logged into Docker CLI, execute the following command: docker login Then enter your DockerHub account and password. Upload Docker Image to DockerHub: Use the following command to push your Docker image to DockerHub: docker push dockerhub_username/image_name:your_tag If you use the previous example, the command will look like this: docker push myusername/myphpimage:latest Check DockerHub: After the upload is complete, you can log into the DockerHub web interface and navigate to your repository to check if the newly uploaded image exists. Please note: Make sure your Docker daemon is already running locally. Before uploading, please test your Docker image to ensure it works properly. Add latest tag to a docker image When you create a Docker image without specifying a tag, Docker automatically labels it as latest. However, if you specify another tag, Docker will not automatically add the latest tag. If you want to add both a specific tag and the latest tag to the same image, you need to perform two tagging and two push operations. Here are the detailed steps: Create the image: docker build -t dockerhub_username/image_name:specific_tag . For example: docker build -t myusername/myphpimage:v1.0 . Tag the image as latest: docker tag dockerhub_username/image_name:specific_tag dockerhub_username/image_name:latest Using the previous example: docker tag myusername/myphpimage:v1.0 myusername/myphpimage:latest Push the image with a specific tag to DockerHub: docker push dockerhub_username/image_name:specific_tag For example: docker push myusername/myphpimage:v1.0 Push the image with the latest tag to DockerHub: docker push dockerhub_username/image_name:latest For example: docker push myusername/myphpimage:latest After completing these steps, your image will have two tags: specific_tag and latest. Clean up docker If you're looking for ways to clean up your Docker environment, below are some commands that can help. Stop all running containers $(docker container ls -a -q) will return all container IDs (including those that have been stopped) docker stop $(docker container ls -a -q) Remove unused resources docker system prune This will remove unused networks, and all stopped containers, all volumes not used by at least one container, and all images without at least one container associated to them. When you run this command, Docker will ask for your confirmation whether or not to delete those resources. If you want not to be asked and you know it is safe to delete, you can add -f for force deletion. Remove all Docker images $(docker images -a -q) will return all image IDs. Please be careful as this may remove images that you are using or planning to use. If you're unsure, it's better not to use this command. docker rmi $(docker images -a -q) Please note the docker system prune command requires Docker 17.06.1-ce or newer. Refer to this log to make sure that your system is up to date! Also, be careful while using these commands as they might delete resources that you might need now or in the future. "},{"url":"Docker/nginx-proxy-manager.html","title":"Nginx Proxy Manager","level":"1.8.3","keywords":[],"body":"NginxProxyManager Setup Create a docker network docker network create scoobydoo Create a docker-compose.yml file version: '3.8' services: npm: image: jc21/nginx-proxy-manager:latest restart: unless-stopped ports: - 80:80 - 81:81 - 443:443 volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt healthcheck: test: ['CMD', '/bin/check-health'] interval: 10s timeout: 3s networks: default: external: true name: scoobydoo Start the container docker-compose up -d Open the web interface http://localhost:81 If you are using headless server, you can use SSH tunneling ssh -L 8181:localhost:81 user@server Then open the web interface http://localhost:8181 Login with the default credentials and change the password Email: admin@example.com Password: changeme Set Reverse Proxy For NginxProxyManager Go to Hosts -> Proxy Hosts -> Add Proxy Host Fill in the form Domain Names: npm.example.com Scheme: http Forward Hostname / IP: npm Forward Port: 81 Remove the default port 81 from docker-compose.yml version: '3.8' services: npm: image: jc21/nginx-proxy-manager:latest restart: unless-stopped ports: - 80:80 - 443:443 volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt healthcheck: test: ['CMD', '/bin/check-health'] interval: 10s timeout: 3s networks: default: external: true name: scoobydoo Restart the container docker-compose down && docker-compose up -d Now if you go to npm.example.com you will see the NginxProxyManager web interface "},{"url":"Docker/portainer.html","title":"Portainer","level":"1.8.4","keywords":[],"body":"Portainer Install protainer via docker-compose Docker-compose Make folder for portainer mkdir portainer && cd portainer docker-compose.yml version: '3.8' services: portainer: image: portainer/portainer-ce:latest container_name: portainer restart: always security_opt: - no-new-privileges:true volumes: - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock:ro - ./portainer-data:/data ports: - 9443:9443 Nginx Create nginx config sudo vim /etc/nginx/sites-available/portainer.example.com Content of portainer.example.com server { listen 80; listen [::]:80; server_name portainer.example.com; location / { proxy_pass https://127.0.0.1:9443; proxy_http_version 1.1; proxy_set_header Host $host; } } Create symbolic link sudo ln -s /etc/nginx/sites-available/portainer.example.com /etc/nginx/sites-enabled/ Check nginx config sudo nginx -t Reload nginx config sudo systemctl reload nginx "},{"url":"Docker/n8n.html","title":"N8N","level":"1.8.5","keywords":[],"body":"N8N Setup Create folder for n8n mkdir n8n && cd n8n Create .env file and set the following variables # Database POSTGRES_USER=admin POSTGRES_PASSWORD=changeme POSTGRES_DB=n8n POSTGRES_NON_ROOT_USER=n8n POSTGRES_NON_ROOT_PASSWORD=changeme # Configure EXECUTIONS_DATA_PRUNE=true EXECUTIONS_DATA_MAX_AGE=168 GENERIC_TIMEZONE=Asia/Taipei WEBHOOK_URL=https://n8n.example.com N8N_HOST=n8n.example.com N8N_PROTOCOL=https N8N_EMAIL_MODE=smtp N8N_SMTP_HOST=smtp.gmail.com N8N_SMTP_PORT=465 N8N_SMTP_USER=example@gmail.com N8N_SMTP_PASS=PASSWORD N8N_SMTP_SENDER=example@gmail.com # N8N_HOST=${SUBDOMAIN}.${DOMAIN_NAME} Create init-data.sh file #!/bin/bash set -e; if [ -n \"${POSTGRES_NON_ROOT_USER:-}\" ] && [ -n \"${POSTGRES_NON_ROOT_PASSWORD:-}\" ]; then psql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" <<-EOSQL CREATE USER ${POSTGRES_NON_ROOT_USER} WITH PASSWORD '${POSTGRES_NON_ROOT_PASSWORD}'; GRANT ALL PRIVILEGES ON DATABASE ${POSTGRES_DB} TO ${POSTGRES_NON_ROOT_USER}; EOSQL else echo \"SETUP INFO: No Environment variables given!\" fi Make init-data.sh executable sudo chmod +x init-data.sh Create docker-compose.yml file version: '3.8' services: postgres: image: postgres:11-alpine restart: always environment: - POSTGRES_USER - POSTGRES_PASSWORD - POSTGRES_DB - POSTGRES_NON_ROOT_USER - POSTGRES_NON_ROOT_PASSWORD volumes: - ./n8n-db:/var/lib/postgresql/data - ./init-data.sh:/docker-entrypoint-initdb.d/init-data.sh healthcheck: test: ['CMD-SHELL', 'pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}'] interval: 5s timeout: 5s retries: 10 n8n: image: n8nio/n8n:latest restart: unless-stopped environment: - DB_TYPE=postgresdb - DB_POSTGRESDB_HOST=postgres - DB_POSTGRESDB_PORT=5432 - DB_POSTGRESDB_DATABASE=${POSTGRES_DB} - DB_POSTGRESDB_USER=${POSTGRES_NON_ROOT_USER} - DB_POSTGRESDB_PASSWORD=${POSTGRES_NON_ROOT_PASSWORD} - EXECUTIONS_DATA_PRUNE=${EXECUTIONS_DATA_PRUNE} - EXECUTIONS_DATA_MAX_AGE=${EXECUTIONS_DATA_MAX_AGE} - GENERIC_TIMEZONE=${GENERIC_TIMEZONE} - WEBHOOK_URL=${WEBHOOK_URL} - N8N_HOST=${N8N_HOST} - N8N_PROTOCOL=${N8N_PROTOCOL} - N8N_EMAIL_MODE=${N8N_EMAIL_MODE} - N8N_SMTP_HOST=${N8N_SMTP_HOST} - N8N_SMTP_PORT=${N8N_SMTP_PORT} - N8N_SMTP_USER=${N8N_SMTP_USER} - N8N_SMTP_PASS=${N8N_SMTP_PASS} - N8N_SMTP_SENDER=${N8N_SMTP_SENDER} expose: - 5678 volumes: - ./n8n-data:/home/node/.n8n depends_on: postgres: condition: service_healthy If you want to make N8N run under same network with NginxProxyManager, you can use the following code to docker-compose.yml version: '3.8' services: postgres: image: postgres:11-alpine restart: always environment: - POSTGRES_USER - POSTGRES_PASSWORD - POSTGRES_DB - POSTGRES_NON_ROOT_USER - POSTGRES_NON_ROOT_PASSWORD volumes: - ./n8n-db:/var/lib/postgresql/data - ./init-data.sh:/docker-entrypoint-initdb.d/init-data.sh healthcheck: test: ['CMD-SHELL', 'pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}'] interval: 5s timeout: 5s retries: 10 networks: - app-tier n8n: image: n8nio/n8n:latest restart: unless-stopped environment: - DB_TYPE=postgresdb - DB_POSTGRESDB_HOST=postgres - DB_POSTGRESDB_PORT=5432 - DB_POSTGRESDB_DATABASE=${POSTGRES_DB} - DB_POSTGRESDB_USER=${POSTGRES_NON_ROOT_USER} - DB_POSTGRESDB_PASSWORD=${POSTGRES_NON_ROOT_PASSWORD} - EXECUTIONS_DATA_PRUNE=${EXECUTIONS_DATA_PRUNE} - EXECUTIONS_DATA_MAX_AGE=${EXECUTIONS_DATA_MAX_AGE} - GENERIC_TIMEZONE=${GENERIC_TIMEZONE} - WEBHOOK_URL=${WEBHOOK_URL} - N8N_HOST=${N8N_HOST} - N8N_PROTOCOL=${N8N_PROTOCOL} - N8N_EMAIL_MODE=${N8N_EMAIL_MODE} - N8N_SMTP_HOST=${N8N_SMTP_HOST} - N8N_SMTP_PORT=${N8N_SMTP_PORT} - N8N_SMTP_USER=${N8N_SMTP_USER} - N8N_SMTP_PASS=${N8N_SMTP_PASS} - N8N_SMTP_SENDER=${N8N_SMTP_SENDER} expose: - 5678 volumes: - ./n8n-data:/home/node/.n8n depends_on: postgres: condition: service_healthy networks: - app-tier - scoobydoo networks: app-tier: driver: bridge scoobydoo: external: true Start the container docker compose up -d Update N8N docker compose pull && docker compose stop && docker compose up -d Reverse Proxy for N8N Create n8n.conf file sudo vim /etc/nginx/sites-available/n8n.conf Add the following code to n8n.conf server { listen 80; listen [::]:80; listen 443 ssl http2; listen [::]:443 ssl http2; server_name n8n.example.com; client_max_body_size 256M; # SSL with Cloudflare ssl_certificate /etc/ssl/certs/cf_example.com.pem; ssl_certificate_key /etc/ssl/private/cf_key_example.com.pem; ssl_client_certificate /etc/ssl/certs/origin-pull-ca.pem; ssl_verify_client on; location / { proxy_pass http://127.0.0.1:5678; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Frame-Options SAMEORIGIN; proxy_buffers 256 16k; proxy_buffer_size 16k; client_max_body_size 50m; client_body_buffer_size 1m; proxy_read_timeout 600s; proxy_buffering off; proxy_cache off; } location ~ ^/(webhook|webhook-test) { proxy_set_header Connection ''; chunked_transfer_encoding off; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Frame-Options SAMEORIGIN; proxy_buffering off; proxy_cache off; proxy_pass http://127.0.0.1:5678; } } Create symbolic link sudo ln -s /etc/nginx/sites-available/n8n.conf /etc/nginx/sites-enabled/n8n.conf Test nginx configuration sudo nginx -t Restart nginx sudo systemctl reload nginx Backup & Restore Docker docker exec -it -u node {CONTAINER_ID} sh Backup Backup workflow n8n export:workflow --all --output=.n8n/workflow.json Backup credential n8n export:credentials --all --decrypted --output=.n8n/credential.json Restore Import workflow n8n import:workflow --input=.n8n/workflow.json Import credential n8n import:credentials --input=.n8n/credential.json "},{"url":"K3s/Setup.html","title":"K3s Setup","level":"1.9.1","keywords":[],"body":"K3s Install To install K3s on a Linux machine, you can follow the official installation guide found on the K3s Documentation Disable swap In order to effectively proceed with the installation and setup of K3s, it's recommended to disable swap on your Linux machine. Kubernetes does not recommend using swap due to potential issues with resource management. Here are the commands to disable swap: Temporarily disable swap: sudo swapoff -a To permanently disable swap, edit the /etc/fstab file and comment out (add # at the beginning) the line that contains the swap partition: #/swapfile swap swap defaults 0 0 The above is an example line. Adjust it according to your system's configuration. After editing, reboot the system. You can now continue with the K3s installation. Setup Install K3s using the script provided by Rancher. This will install and configure a lightweight Kubernetes cluster. curl -sfL https://get.k3s.io | sh - To specify an alternate version or additional options, you can customize the above command as needed. For example, to install K3s without the default traefik ingress controller, you can use the following command: curl -sfL https://get.k3s.io | sh -s - server \\ --tls-san YOUR_SERVER_IP \\ --node-external-ip YOUR_EXTERNAL_IP \\ --disable traefik \\ --cluster-init Verify Installation After the installation is complete, you can verify the status of the K3s service using the following command: sudo systemctl status k3s If the service is running, you should see an output indicating that the service is active and running. Check Pods You can also check the status of the pods in the kube-system namespace to ensure that the core components of K3s are running correctly: kubectl get pods -n kube-system Work with firewall Disable UFW (Uncomplicated Firewall) For a straightforward setup of K3s, it is common to disable UFW to avoid any issues with network traffic and port access. If you choose this approach, execute the following: sudo ufw disable Disabling UFW will allow all network traffic, facilitating K3s without additional configuration. Configure UFW for K3s If you prefer to keep UFW enabled for added security, you must ensure that K3s can communicate using specific ports. Here are the essential steps and rules to enable: Allow SSH Traffic: Before enabling UFW, ensure that SSH traffic is allowed to maintain remote access: sudo ufw allow OpenSSH This command allows SSH on the default port 22. If your SSH uses a different port, be sure to specify it accordingly (e.g., sudo ufw allow 2222/tcp for port 2222). Enable UFW: After ensuring SSH is allowed, you can safely enable UFW: sudo ufw enable Allow K3s Default Ports: K3s typically uses several ports for cluster communication and applications. Make sure these ports are open: # Allow standard Kubernetes API server port sudo ufw allow 6443/tcp #apiserver sudo ufw allow from 10.42.0.0/16 to any #pods sudo ufw allow from 10.43.0.0/16 to any #services Other ports that may be required include: # Required only if using HA with embedded etcd sudo ufw allow 2379/tcp # etcd server client API sudo ufw allow 2380/tcp # etcd server communication # Enable kubelet metrics traffic between all nodes sudo ufw allow 10250/tcp # Flannel VXLAN - required only if using the Flannel VXLAN backend sudo ufw allow 8472/udp # Required only for Flannel WireGuard backend sudo ufw allow 51820/udp # For Flannel WireGuard with IPv4 sudo ufw allow 51821/udp # For Flannel WireGuard with IPv6 # Required only for embedded distributed registry (Spegel) sudo ufw allow 5001/tcp Adjust the rules based on your specific requirements and configurations. Verify UFW Status: After configuring the rules, verify the status to ensure the correct ports are allowed: sudo ufw status By selectively allowing these ports, K3s can function while maintaining UFW‚Äôs protection for other unnecessary traffic. Always ensure that only the necessary ports and services are exposed to avoid security risks. Use K3s Without Sudo Configuring your system to use kubectl commands without sudo involves setting up the KUBECONFIG environment variable. Set the KUBECONFIG Environment Variable: First, set up the KUBECONFIG environment variable in your current shell session to point to your local configuration file: export KUBECONFIG=~/.kube/config Generate the Local Configuration File: Create the .kube directory and generate the configuration file using the following command: mkdir -p ~/.kube sudo k3s kubectl config view --raw > \"$KUBECONFIG\" chmod 600 \"$KUBECONFIG\" This ensures that your Kubernetes configuration file is only readable by your user, maintaining security. Persist Configuration Across Reboots: To ensure the KUBECONFIG environment variable is automatically set in every new terminal session, add it to your ~/.profile or ~/.bashrc file: echo 'export KUBECONFIG=~/.kube/config' >> ~/.bashrc source ~/.bashrc If you use a different shell (e.g., zsh), make similar additions in your ~/.zshrc. By setting it up this way, you can run kubectl without needing sudo, while maintaining proper security for your k3s.yaml file. "},{"url":"K3s/Uninstall.html","title":"K3s Uninstall","level":"1.9.2","keywords":[],"body":"How to Remove K3s Stop K3s Services To completely remove K3s from your Linux system, ensure that all related services are stopped and configurations are cleared. Stop All K3s Services: Before uninstalling, use the k3s-killall.sh script to stop any running K3s services and processes. sudo /usr/local/bin/k3s-killall.sh Uninstall K3s Removing K3s from your Linux system is straightforward. Follow these steps to ensure a clean removal of the lightweight Kubernetes distribution. Uninstall K3s: The simplest way to remove K3s is by using the uninstall script provided during the installation. This script will remove all K3s components and configurations. /usr/local/bin/k3s-uninstall.sh Verify Removal: Ensure all K3s services and configurations are removed: Check Services: Confirm no K3s services are running. systemctl status k3s Remove Directories: Check and manually remove any leftover directories related to K3s if necessary, such as: sudo rm -rf /etc/rancher/k3s Remove CLI Alias: If you used aliases for K3s commands, ensure to remove them from your shell configuration files (e.g., .bashrc, .zshrc). Remove Environment Variables: If you set any environment variables, such as KUBECONFIG, to point to your K3s configuration, be sure to unset or remove these from your shell configuration files. By following these steps, you can completely remove K3s from your system, leaving it ready for any new installations or configurations. If you encounter any issues, refer to the K3s Documentation for more detailed troubleshooting steps. "},{"url":"K3s/CommandList.html","title":"K3s Command List","level":"1.9.3","keywords":[],"body":"Common K3s Commands Here's a guide on how to manage your K3s cluster, including starting services, checking the status of nodes and pods, stopping services, and adding nodes. Additionally, we'll cover the usage of kubectl apply to manage deployments. Start K3s Service To start the K3s service if it's not already running, use: sudo systemctl start k3s Check K3s Service Status To check the status of the K3s service, use: sudo systemctl status k3s Check Cluster Status You can check the status of your nodes, pods, and services using the following kubectl commands: View Nodes: This command shows the list and status of all nodes in your cluster. kubectl get nodes -o wide View Pods: To list all pods in the default namespace or specify another namespace with -n <namespace>: kubectl get pods -o wide View Services: Lists all services running in the default namespace, or specify another namespace. kubectl get svc -o wide Deploy Applications To deploy an application using a YAML configuration file such as nginx-deployment.yml: Apply Deployment: This command will create or update resources defined in the YAML file, handling the deployment of pods and services. kubectl apply -f nginx-deployment.yml Note: kubectl apply will ensure that services and pods are restarted and updated automatically as defined in the YAML file. Delete Deployment: To delete a deployment, use the following command: kubectl delete -f nginx-deployment.yml Note: This command will delete the resources defined in the YAML file. Check Deployment Status To check the status of a deployment, use the following command: kubectl get deployments -o wide This command will show the status of all deployments in the default namespace. Scale Deployments To scale a deployment, use the following command: kubectl scale deployment <deployment-name> --replicas=<number-of-replicas> This command will scale the deployment to the specified number of replicas. Update Deployments To update a deployment with a new image or configuration, use: kubectl set image deployment/<deployment-name> <container-name>=<new-image> This command will update the deployment with the new image. Delete Deployments To delete a deployment, use the following command: kubectl delete deployment <deployment-name> This command will delete the specified deployment. Stop K3s Service To stop the K3s service, if needed: sudo systemctl stop k3s Add a Node to the K3s Cluster To add a new node to your existing K3s cluster, perform the following: Get the Token: Retrieve the node token from the server, which is required for joining nodes: sudo cat /var/lib/rancher/k3s/server/node-token Install K3s on the New Node: Run the K3s installation script with the server's IP address and token to join the cluster: curl -sfL https://get.k3s.io | K3S_URL=https://<server-ip>:6443 K3S_TOKEN=<node-token> K3S_NODE_NAME=worker-1 sh - Verify Node Joining: Check the status of the new node to ensure it has joined the cluster: kubectl get nodes -o wide Label the Node (Optional): If needed, you can label the new node for specific workloads: kubectl label node <node-name> <label-key>=<label-value> For example, to label a node as a worker: kubectl label node <node-name> node-role.kubernetes.io/worker=worker Remove the label with: kubectl label node <node-name> <label-key>- For example, to remove the worker label: kubectl label node <node-name> node-role.kubernetes.io/worker- Generate base64 Encoded K3s Config File To generate a base64 encoded K3s config file for use with kubectl: cat ~/.kube/config | base64 -w 0 By following these commands and procedures, you can effectively manage your K3s cluster, ensuring smooth deployment and scaling of your applications. For further details on each command, consult the Kubernetes kubectl Documentation. "},{"url":"K3s/Helm.html","title":"Helm","level":"1.9.4","keywords":[],"body":"Helm Description Helm is a package manager for Kubernetes. It is a tool that streamlines installing and managing Kubernetes applications. Think of it like apt/yum/homebrew for Kubernetes. Installation Adding Helm GPG Key to Keyrings curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null curl https://baltocdn.com/helm/signing.asc: Downloads the Helm signing key using curl. | gpg --dearmor: Pipes the downloaded key into gpg to convert the ASCII key into a binary format. | sudo tee /usr/share/keyrings/helm.gpg > /dev/null: Writes the binary key to /usr/share/keyrings/helm.gpg, effectively adding the Helm keyring to the system's list of trusted keys. > /dev/null sends any output messages to a null device, suppressing them from appearing in the terminal. Install Transport over HTTPS sudo apt install apt-transport-https --yes sudo apt install apt-transport-https --yes: Ensures that the package apt-transport-https is installed, which is required for apt to retrieve packages over HTTPS. The --yes flag automatically confirms the installation without user interaction. Adding Helm Repository to Sources List echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\": Constructs a new source list entry. The architecture is dynamically set using $(dpkg --print-architecture), and the entry is signed by the previously added Helm GPG key. | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list: Writes the new source list entry to /etc/apt/sources.list.d/helm-stable-debian.list, making the Helm repository available to apt. Updating Package List sudo apt update sudo apt update: Updates the local package index to include the new Helm repository, ensuring that apt is aware of the latest packages available for installation. Installing Helm sudo apt install helm sudo apt install helm: Installs Helm using apt, now that the package index has been updated to include Helm's repository. These steps collectively configure the system to recognize the Helm package repository and ultimately install Helm using the system's package manager. "},{"url":"K3s/MetalLB.html","title":"MetalLB","level":"1.9.5","keywords":[],"body":"MetalLB Install To set up MetalLB in your K3s environment, follow these steps: First, create a namespace for MetalLB: kubectl create namespace metallb-system Next, use the Helm chart to install MetalLB. Make sure you have added the MetalLB repository to Helm and updated it: helm repo add metallb https://metallb.github.io/metallb helm repo update Now, install MetalLB in the metallb-system namespace: helm install metallb metallb/metallb --namespace metallb-system This will install all the necessary MetalLB components into your K3s environment. Configuration Once MetalLB is installed, you need to configure an IP address pool from which it can allocate addresses for LoadBalancer services. Create a configuration file metallb-config.yml with an IP range that matches your network setup: apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: namespace: metallb-system name: my-ip-pool spec: addresses: - 192.168.32.200-192.168.32.220 # Adjust IP range as per your local setup --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: namespace: metallb-system name: my-l2-advertisement spec: ipAddressPools: - my-ip-pool Apply the configuration: kubectl apply -f metallb-config.yml Verify Installation Check MetalLB Pods: Ensure all MetalLB pods are running: kubectl get pods -n metallb-system Test LoadBalancer: Deploy a simple service with LoadBalancer type to test MetalLB: apiVersion: v1 kind: Service metadata: name: test-loadbalancer spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 type: LoadBalancer Apply and observe the EXTERNAL-IP: kubectl apply -f test-loadbalancer.yml kubectl get services Networking Notes When using MetalLB: Ensure the IP address range you specify does not conflict with any other devices on the network. If using Layer 2 mode, all nodes in the cluster must be in the same Layer 2 domain (i.e., connected to the same switch/bridging network). Enable Logging For troubleshooting, you can enable logging on MetalLB to see allocation activity: kubectl logs <metallb-controller-pod-name> -n metallb-system kubectl logs <metallb-speaker-pod-name> -n metallb-system Replace <metallb-controller-pod-name> and <metallb-speaker-pod-name> with the actual pod names using kubectl get pods. Setting up MetalLB in your K3s environment allows you to simulate cloud-like external IP addressing, optimizing your configurations for a multi-server deployment. "},{"url":"K3s/Istio.html","title":"Istio","level":"1.9.6","keywords":[],"body":"Istio This guide provides step-by-step instructions for installing the Gateway API CRDs and Istio on your Kubernetes cluster, leveraging the built-in support for Gateway API to manage ingress traffic. Prerequisites Ensure any platform-specific setup is complete. Verify the requirements for Pods and Services in your Kubernetes cluster. Install the latest Helm client. Note that Helm versions released before the oldest currently-supported Istio release are neither tested nor recommended. Gateway API CRD Installation To install Gateway API CRDs, execute the following command: kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/latest/download/standard-install.yaml Istio Installation Follow these steps to install Istio using Helm charts: Configure the Helm Repository To add and update the Istio Helm repository, use: helm repo add istio https://istio-release.storage.googleapis.com/charts helm repo update Install the Istio Base Chart The Istio base chart contains cluster-wide Custom Resource Definitions (CRDs) that must be installed before deploying the Istio control plane. Install it using: helm install istio-base istio/base -n istio-system --set defaultRevision=default --create-namespace Validate the installation: helm ls -n istio-system Ensure the istio-base status is deployed. Install the Istio Discovery Chart Deploy the Istio control plane: helm install istiod istio/istiod -n istio-system --wait Verify the installation: helm ls -n istio-system Check the status to ensure istiod is deployed: helm status istiod -n istio-system Check istiod service installation: kubectl get deployments -n istio-system --output wide Configuring and Using Gateway API With Gateway API CRDs installed, define your Gateway and HTTPRoute resources. This will automatically provision gateway deployments and services based on the Gateway configurations. Refer to the Gateway API documentation for defining: Gateway: Resource defining how ingress traffic is handled. HTTPRoute: Specifies routing rules to route traffic based on HTTP properties. When Using Gateway API CRDs, You Can Omit: Optional Step: Install an Ingress Gateway To install an ingress gateway, create a separate namespace and deploy the chart: kubectl create namespace istio-ingress helm install istio-ingress istio/gateway -n istio-ingress --wait Ensure the namespace does not have an istio-injection=disabled label for the gateway to function properly. Updates and Advanced Customization To update Istio configurations or perform advanced customization, refer to the available configuration options using: helm show values istio/<chart-name> Uninstall Istio To remove Istio from your cluster, follow these steps: Check installed Istio charts: helm ls -n istio-system (Optional) Delete any ingress gateway installations: helm delete istio-ingress -n istio-ingress kubectl delete namespace istio-ingress Remove Istio discovery chart: helm delete istiod -n istio-system Remove Istio base chart: helm delete istio-base -n istio-system Delete istio-system namespace: kubectl delete namespace istio-system Ensure you understand the implications of each command before execution in your production environment. "},{"url":"K3s/Longhorn.html","title":"Longhorn","level":"1.9.7","keywords":[],"body":"Longhorn Install To set up Longhorn in your K3s environment using Helm, follow these steps: First, you should create a namespace for Longhorn: kubectl create namespace longhorn-system Next, add the Longhorn Helm repository to your Helm setup and update it: helm repo add longhorn https://charts.longhorn.io helm repo update Now, you can install Longhorn in the longhorn-system namespace: helm install longhorn longhorn/longhorn -n longhorn-system This command will deploy all necessary Longhorn components in your K3s environment. Configuration After installing Longhorn, you can configure it to suit your specific storage needs. Longhorn automatically provides a storage class named longhorn, which you can use to create PersistentVolumeClaims (PVCs). Replica Setting Warning: The UI's setting for replica is for UI created volume. The default number of replicas when creating the volume from Longhorn UI. For Kubernetes, update the numberOfReplicas in the StorageClass. Kubernetes doesn't allow modify the parameter of the storage class. So you can create another storage class with Longhorn as driver, and set replica count to 1 on that. It's not recommended to modify the default storage class since it might cause issues during the upgrade. To define a PVC using Longhorn, create a YAML file: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: longhorn-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: longhorn Apply this configuration to create a PVC: kubectl apply -f longhorn-pvc.yml Verify Installation Check Longhorn Pods: Ensure all Longhorn components are running: kubectl get pods -n longhorn-system Access the Longhorn UI: To use Longhorn's UI for management, use port forwarding to access it locally: kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80 Then visit http://localhost:8080 in your web browser. Testing Longhorn To test Longhorn, you can deploy a simple application that uses the PVC: apiVersion: apps/v1 kind: Deployment metadata: name: test-longhorn spec: replicas: 1 selector: matchLabels: app: test-longhorn template: metadata: labels: app: test-longhorn spec: containers: - name: nginx image: nginx:alpine volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: storage volumes: - name: storage persistentVolumeClaim: claimName: longhorn-pvc Apply this configuration and verify that the application is using Longhorn storage: kubectl apply -f test-longhorn.yml kubectl get pods Enable Monitoring and Alerting Longhorn integrates well with Prometheus and Grafana. Consider setting these up for monitoring Longhorn's usage statistics and performance metrics. With Longhorn installed, you now have a robust and reliable storage solution that's well integrated into your K3s cluster, enhancing your storage capabilities and management. If you require further customization or encounter any issues, the community and Longhorn documentation provide excellent resources. "},{"url":"K3s/Secret.html","title":"Secret","level":"1.9.8","keywords":[],"body":"Secrets in K3s Note: In this guide, if the -n (namespace) option is omitted, the default namespace (default) will be used automatically. Create a Secret Create a Secret from a Literal Value: Use the kubectl command to create a secret directly from the command line with literal values: kubectl create secret generic my-secret --from-literal=username=myuser --from-literal=password=mypassword -n my-namespace Create a Secret from a File: You can create a secret from files containing sensitive information: kubectl create secret generic my-file-secret --from-file=config.json=/path/to/config.json -n my-namespace View a Secret To view the details of a secret with base64-encoded data: kubectl get secret my-secret -n my-namespace -o yaml Decode a specific piece of secret data: kubectl get secret my-secret -n my-namespace -o jsonpath=\"{.data.username}\" | base64 --decode Manage/Update a Secret Use these methods to update secrets: Edit the Secret Directly: Be cautious as the data is base64 encoded: kubectl edit secret my-secret -n my-namespace Recreate/Update a Secret: Delete the old secret and create it again with new data: kubectl delete secret my-secret -n my-namespace kubectl create secret generic my-secret --from-literal=username=newuser --from-literal=password=newpassword -n my-namespace Delete a Secret Delete a secret, ensuring the correct namespace is specified: kubectl delete secret my-secret -n my-namespace Verify a Secret List secrets to confirm existence in the intended namespace: kubectl get secrets -n my-namespace Namespace Considerations Always specify -n to target the desired namespace, preventing unintended interactions with other resources. If necessary, create namespaces prior to secret operations using: kubectl create namespace my-namespace By understanding these commands, you'll effectively manage your Kubernetes secrets, ensuring sensitive data is securely handled within your cluster environments. Creating a cloudflare-tls Secret In this example, we will create a secret called cloudflare-tls to store TLS certificates and keys. Make sure to specify the namespace using -n, or it will default to the default namespace if omitted. Prerequisites Ensure that you have your TLS certificate (tls.crt) and private key (tls.key) files ready. Create the TLS Secret Use the following command to create a TLS type secret. This secret type is specifically for holding SSL/TLS certificates and keys: Store TLS Certificate and Key: Use the kubectl command to create the secret, specifying the files for the certificate and key: kubectl create secret tls cloudflare-tls \\ --cert=path/to/tls.crt \\ --key=path/to/tls.key \\ -n my-namespace Verify the Secret To confirm that the cloudflare-tls secret was created successfully, and to view its basic details: kubectl get secret cloudflare-tls -n my-namespace -o yaml This will show the created secret details in YAML format, where the certificate and key are stored in a base64-encoded form. Use the cloudflare-tls Secret You can reference this secret in your Kubernetes resources, such as Ingress or a specific application that requires TLS configuration, by using its name and namespace. Namespace Considerations Always ensure you specify -n my-namespace to correctly place the secret in the desired namespace. If working with multiple environments, carefully manage secrets across different namespaces to avoid exposure or accidental overwriting. By following these steps, you can securely manage your TLS secrets within your Kubernetes environment. If you need further clarification or examples, let me know! "},{"url":"K3s/CertManager.html","title":"Cert-Manager","level":"1.9.9","keywords":[],"body":"Cert-Manager Install Cert-Manager To install Cert-Manager using Helm in your K3s cluster, follow these steps: Add the Jetstack Helm Repository: You'll need to add the Jetstack Helm repository to get the Cert-Manager charts. helm repo add jetstack https://charts.jetstack.io helm repo update Create a Namespace for Cert-Manager: It is recommended to install Cert-Manager in its own namespace, cert-manager. kubectl create namespace cert-manager Install Cert-Manager with Helm: Use Helm to install Cert-Manager into the cert-manager namespace. helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --set crds.enabled=true Verify Installation: Ensure that all Cert-Manager pods are up and running: kubectl get pods -n cert-manager Verify Installation To fully verify the Cert-Manager installation, the best approach is to issue a test certificate. This involves creating a self-signed issuer and a certificate resource in a test namespace. Follow these steps: Create Test Resources: Create a YAML file called test-resources.yaml with the following content: apiVersion: v1 kind: Namespace metadata: name: cert-manager-test --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-cert namespace: cert-manager-test spec: dnsNames: - example.com secretName: selfsigned-cert-tls issuerRef: name: test-selfsigned Apply the Test Resources: Apply the configuration to create the namespace, issuer, and certificate: kubectl apply -f test-resources.yaml Check the Certificate Status: After applying, check the status of the newly created certificate. You might need to wait a few seconds for cert-manager to process the certificate request. kubectl describe certificate selfsigned-cert -n cert-manager-test You should see something like this in the output: Spec: Common Name: example.com Issuer Ref: Name: test-selfsigned Secret Name: selfsigned-cert-tls Status: Conditions: Last Transition Time: YYYY-MM-DDTHH:MM:SSZ Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: YYYY-MM-DDTHH:MM:SSZ Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CertIssued 4s cert-manager Certificate issued successfully Clean Up the Test Resources: Once you've verified that the certificate is issued successfully, you can clean up the resources: kubectl delete -f test-resources.yaml If all the above steps complete without any errors, your Cert-Manager installation is functioning properly! Create a Certificate Issuer with Let's Encrypt Set Up an Issuer or ClusterIssuer: You'll need to create an Issuer or ClusterIssuer for your cluster to get certificates. Here's an example configuration for a ClusterIssuer using Let's Encrypt: apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: your-email@example.com privateKeySecretRef: name: letsencrypt-staging solvers: - http01: ingress: class: nginx Apply the issuer configuration: kubectl apply -f issuer.yml Request a Certificate: Deploy a test application with a certificate request to confirm Cert-Manager is functioning correctly. You may create a Certificate resource and check if it gets issued. Debugging and Logs: Utilize logs for troubleshooting issues with Cert-Manager: kubectl logs -l app=cert-manager -n cert-manager Verify Certificates: Inspect the Certificate resources and verify Secrets issued: kubectl get certificates -A kubectl get secrets -A | grep your-certificate-name Create a Certificate Issuer with Cloudflare This guide will walk you through setting up a Cloudflare-based issuer with Cert-Manager on your K3s cluster. It builds upon the same structure shown in the previous example but replaces the issuer configuration with Cloudflare-specific settings. We assume that you have already: Installed Cert-Manager using Helm. Configured your K3s cluster properly. Below are the steps to create a secret for your Cloudflare API token and a ClusterIssuer that uses DNS-01 validation. 1. Create a Cloudflare API Token Log into Cloudflare:Go to the Cloudflare Dashboard and navigate to My Profile ‚Üí API Tokens. Generate a Token:Create an API token with DNS:Edit, Zone:Read (or broader) permission for the specific zone you will be validating. Copy the Token:After creating the token, copy it somewhere safe (e.g., abc123xyz...). You will need it in the next step. 2. Create a Kubernetes Secret for the Cloudflare Token You need to store your Cloudflare API token as a secret in your cluster. If Cert-Manager is installed in the cert-manager namespace, you can place the secret there as well: kubectl create secret generic cloudflare-api-token-secret \\ --from-literal=api-token='abc123xyz...' \\ -n cert-manager Secret Name: cloudflare-api-token-secret Key: api-token Alternatively, you can create this secret via a YAML manifest: apiVersion: v1 kind: Secret metadata: name: cloudflare-api-token-secret namespace: cert-manager type: Opaque data: # Base64-encoded token string api-token: <BASE64_ENCODED_CLOUDFLARE_TOKEN> Note: Make sure your API token is base64-encoded if you choose the YAML approach. For example: echo -n \"abc123xyz...\" | base64 3. Create a ClusterIssuer for Cloudflare Cert-Manager uses a ClusterIssuer (or Issuer) to obtain certificates. Below is an example ClusterIssuer that uses Let‚Äôs Encrypt production endpoint (you can also switch to the staging endpoint for testing). apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: cloudflare-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: your-email@example.com privateKeySecretRef: name: cloudflare-issuer-account-key solvers: - dns01: cloudflare: # Reference your secret and the key where the token is stored apiTokenSecretRef: name: cloudflare-api-token-secret key: api-token server: Points to the Let‚Äôs Encrypt ACME URL. Production: https://acme-v02.api.letsencrypt.org/directory Staging: https://acme-staging-v02.api.letsencrypt.org/directory email: The email address associated with your Let‚Äôs Encrypt account. privateKeySecretRef.name: The Secret where Cert-Manager will store your ACME account‚Äôs private key. You can name this Secret as you like (e.g., cloudflare-issuer-account-key). apiTokenSecretRef.name: The name of the secret you created in the previous step (cloudflare-api-token-secret). apiTokenSecretRef.key: The key in that secret where the token is stored (default: api-token). Once you‚Äôve created the YAML file (e.g., cloudflare-issuer.yaml), apply it: kubectl apply -f cloudflare-issuer.yaml 4. Verify Your Cloudflare Issuer Check the status of your ClusterIssuer: kubectl describe clusterissuer cloudflare-issuer Look for Status: True or any events indicating whether your issuer is ready. If you see error messages, consult the Cert-Manager controller logs: kubectl logs -l app=cert-manager -n cert-manager 5. Request a Certificate Once the issuer is ready, you can request a certificate for a specific domain. For example, to get a certificate for example.adakrei.com, create a Certificate manifest like the following: apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: example-adakrei-com-cert namespace: default spec: secretName: example-adakrei-com-tls issuerRef: name: cloudflare-issuer kind: ClusterIssuer dnsNames: - example.adakrei.com Apply this file: kubectl apply -f certificate.yaml Cert-Manager will perform a DNS-01 challenge using your Cloudflare API token to create a _acme-challenge.example.adakrei.com TXT record. After successful validation, a TLS certificate will be issued and stored in the example-adakrei-com-tls secret. Use the following commands to verify: kubectl describe certificate example-adakrei-com-cert kubectl get secret example-adakrei-com-tls You should see the secret containing tls.crt and tls.key if everything is working properly. Summary Install Cert-Manager (already completed). Create a Secret for your Cloudflare API token (cloudflare-api-token-secret). Create a ClusterIssuer referencing the Cloudflare token (cloudflare-issuer). Request a Certificate to confirm correct DNS-01 validation. Use the TLS Secret in your Ingress, Gateway, or HTTPRoute to enable HTTPS. With these steps, you should have a fully functioning Cloudflare-based issuer in your K3s cluster, enabling Cert-Manager to automatically issue and renew certificates via Let‚Äôs Encrypt. If you encounter issues, check the Cert-Manager logs and ensure your Cloudflare API token has the necessary DNS edit permissions for the relevant zone. Uninstall Cert-Manager To uninstall Cert-Manager from your K3s cluster, follow these steps: Delete the Cert-Manager Release: Use Helm to delete the Cert-Manager release: helm delete cert-manager -n cert-manager Delete the Cert-Manager Namespace: Remove the cert-manager namespace: kubectl delete namespace cert-manager By following these steps, you can install Cert-Manager in your K3s cluster using Helm. Ensure your Kubernetes environment is properly prepared and configured for Cert-Manager to issue and manage certificates. "},{"url":"K3s/ExternalDNS.html","title":"ExternalDNS","level":"1.9.10","keywords":[],"body":"ExternalDNS Install ExternalDNS To integrate ExternalDNS with your K3s cluster using Cloudflare, follow these steps: Create Kubernetes Secret for Cloudflare API Token: You'll need a Cloudflare API token with the necessary permissions (Zone:Read, DNS:Edit). Replace YOUR_API_TOKEN with your Cloudflare API token. kubectl create secret generic cloudflare-api-key --from-literal=apiKey=YOUR_API_TOKEN -n kube-system Create values.yml for Helm Configuration: Create a values.yml file to configure ExternalDNS with Cloudflare as the DNS provider: provider: name: cloudflare env: - name: CF_API_TOKEN valueFrom: secretKeyRef: name: cloudflare-api-key key: apiKey Add and Install ExternalDNS with Helm: Add the ExternalDNS Helm repository and update it: helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/ helm repo update Now, install ExternalDNS in the kube-system namespace: helm upgrade --install external-dns external-dns/external-dns --namespace kube-system --values values.yml Verify Installation Check ExternalDNS Pods: Ensure the ExternalDNS pods are running by executing: kubectl get pods -n kube-system | grep external-dns Configure a Service with DNS Annotations: To test ExternalDNS, deploy a service annotated with the desired hostname. Example for an Nginx service: apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx annotations: external-dns.alpha.kubernetes.io/hostname: example.com external-dns.alpha.kubernetes.io/ttl: \"120\" #optional external-dns.alpha.kubernetes.io/target: \"YOUR_PUBLIC_IP\" #optional external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\" #optional spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 Apply the configuration and check for the assigned EXTERNAL-IP: kubectl apply -f nginx-service.yml kubectl get services Networking Notes Cloudflare Configuration: Ensure the DNS zone in Cloudflare is correctly set up and the API token has the necessary permissions. IP Address Consideration: If using MetalLB, ensure the EXTERNAL-IP assigned by MetalLB to a service does not conflict with other network devices. Enable Logging To troubleshoot any issues with ExternalDNS, enable logging by observing the ExternalDNS pod logs: kubectl logs deployment/external-dns -n kube-system This setup allows ExternalDNS to dynamically manage DNS records for services exposed via a LoadBalancer in your K3s environment, aligning with a Cloudflare configuration for easy DNS management. Key Considerations and Common Issues Service Type Matters: LoadBalancer: ExternalDNS typically requires services to have the type LoadBalancer. This is because a LoadBalancer assigns an external IP that ExternalDNS can use to update DNS records. In a cloud environment, this usually results in a publicly accessible IP. NodePort: If your environment does not support LoadBalancer and you are using NodePort, ensure you manually associate the Node's external IP with the DNS name in Cloudflare. This requires that you know the external IPs of the nodes and appropriately handle traffic routing. IP Address Visibility: A service of type LoadBalancer or NodePort needs to expose a public or routable IP for ExternalDNS to work effectively. In environments like MetalLB, ensure the IP range used does not conflict with local network devices. MetalLB and Internal IP Issues: When using MetalLB, services configured as LoadBalancer can end up with internal IPs (e.g., 192.168.x.x). While useful for local testing, these are not suitable for global DNS exposure. To expose services externally with MetalLB, consider setting up a NAT or reverse proxy for external access, and then manually adjust DNS entries or use a different cloud provider or infrastructure that provides public IPs natively. DNS Annotations: Ensure that services meant to be exposed via ExternalDNS have the correct DNS annotations, such as: metadata: annotations: external-dns.alpha.kubernetes.io/hostname: \"your.domain.com\" Cloudflare Configuration: Ensure the Cloudflare API token has the proper permissions and the domain configured in your annotations matches the zones managed in Cloudflare. Verification Steps Pod and Service Status: Before assuming ExternalDNS configuration issues, verify that all involved Kubernetes resources, such as Deployments, Services, and the ExternalDNS deployment itself, are running and correctly configured. kubectl get pods -n kube-system | grep external-dns kubectl get services Logs and Debugging: Check logs for ExternalDNS for insight into any DNS updates or errors: kubectl logs deployment/external-dns -n kube-system Cloudflare Dashboard: Confirm that DNS records are updated in Cloudflare by checking the domain's DNS settings. Ensure the ExternalDNS service has updated these records corresponding to the correct IP addresses exposed by your services. Setting ExternalDNS correctly, especially with the combination of MetalLB and internal K3s environments, demands careful handling of network configurations and service types. For production-grade setups, ensure robust IP management and DNS adjustments that reflect real-world accessibility needs. BGP vs. NAT: Best Practices in Different Network Scenarios In on-prem or private data center environments, if you want services within a K8s cluster to have \"true\" public IPs, there are generally two approaches: NAT / Forwarding (Single IP / Few IPs) Applicable Scenarios: You have one or a few public IPs, the ISP has not provided you with an announcable IP block, or the equipment (such as FortiGate) has not enabled BGP. Approach: Use DNAT / Port Forwarding on the firewall or router (e.g., FortiGate) to forward external 80/443 traffic to the cluster's MetalLB IP or NodePort. ExternalDNS Configuration: You need to manually point the annotation to the public IP (external firewall IP). Otherwise, ExternalDNS might automatically read a private IP. Pros and Cons: Pros: Simple deployment, no need for ISP cooperation, or BGP environment. Cons: If multiple services share the same IP, multiple Port Forwards are needed on FortiGate or use the same IP protocol to achieve multiple virtual hosts. BGP Exchange (Multiple / Announcable IP Block) Applicable Scenarios: You have multiple public IPs (e.g., /29, /28 blocks) and can perform BGP Peering with the ISP or upstream equipment, with an ASN or route announcement permission. Approach: Install MetalLB and enable BGP mode, letting MetalLB be the BGP Speaker to announce the public IP assigned to a service directly to the router or upstream network. ExternalDNS Configuration: ExternalDNS can automatically write the public IP assigned by MetalLB into DNS. No manual override needed. Pros and Cons: Pros: Each service gets a truly independent public IP without NAT; a cleaner traffic path; scalable expansion. Cons: Requires network equipment to support BGP and ownership of announcable IP blocks; configuration and maintenance are relatively complex. Conclusion: If you only have one public IP, or cannot use BGP, NAT with ExternalDNS manual override or Ingress/Gateway with internal IPs are common practices. If you have sufficient public IP blocks and network equipment/ISP supports BGP, then BGP is a more \"standard\" and efficient approach, avoiding cumbersome manual NAT and DNS overrides. After setting up ExternalDNS correctly, especially when combined with MetalLB in on-prem environments, it's crucial to check external IPs, DNS records, and firewall / route forwarding. Using BGP on ISP or enterprise networks allows each service to obtain a real public IP; otherwise, in a single IP + NAT environment, manual annotations can direct ExternalDNS to the firewall's external IP, with the firewall forwarding the traffic. These adjustments must be tailored to actual network conditions. "},{"url":"Server/","title":"Server","level":"1.10","keywords":[],"body":"Server bbr.sh Description: BBR Easy Setup Tool Support OS: Debian 6+ / Ubuntu 14+ Installation: WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Server/bbr.sh && chmod +x bbr.sh && bash bbr.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Server/bbr.sh && chmod +x bbr.sh && bash bbr.sh "},{"url":"GCP/SSH.html","title":"Connect GCP with SSH Key","level":"1.11.1","keywords":[],"body":"Connect GCP With Personal SSH Key Connecting to an Instance via CLI You can connect to the instance via CLI in two ways - through gcloud commands or through SSH commands. Here we will primarily discuss SSH. Using Existing Public/Private Keys Usually, public and private keys are stored in the ~/.ssh directory. The files will appear in pairs with names my-ssh-key.pub and my-ssh-key, where my-ssh-key could be any filename. > Note: In practice, my-ssh-key is by default named id_rsa and the server reads id_rsa as the key. If you want to use another key file, remember to use the -i flag. If you have existing public and private keys in your files, you can directly obtain the public key through cat my-ssh-key.pub and store this public key in the Metadata SSH keys. You can then connect by using the following command ssh -i ~/.ssh/my-ssh-key [USERNAME]@[IP_ADDRESS] This will establish the connection. Generating New Public/Private Keys You can generate a new pair of public/private keys using the following command: ssh-keygen -t rsa -f ~/.ssh/my-ssh-key -C [USERNAME] Input a passphrase to protect this pair of keys. You can restrict access to the key pair to yourself: chmod 400 ~/.ssh/my-ssh-key Next, obtain your public key through cat ~/.ssh/my-ssh-key.pub Add this key to the Metadata SSH keys and connect via ssh -i ~/.ssh/my-ssh-key [USERNAME]@[IP_ADDRESS] SSH Command When we use the SSH command, we connect via ssh -i ~/.ssh/my-ssh-key [USERNAME]@[IP_ADDRESS] > Note1: The username is the username of your Gmail account (username@gmail.com)Note2: You can find the ip_address in the instance details. Checking Whether Your Key is Stored on Google Cloud If you go to 'Metadata' in the menu now, and click on the SSH keys, ideally you should see the following interfaceThis means that Google Compute Engine has stored your key. If you hadn't created any instances in this project and made a connection, essentially there would be no key stored. You can check the file content in the ~/.ssh directory, and you should see google_compute_engine.pub and google_compute_engine files. These two files represent the public key (pub) and private key generated by Google Cloud for you. The one stored on Google Cloud is the public key (google_compute_engine.pub). You can check the content of your public key by typing in the terminal cat google_compute_engine.pub The content should be consistent with that stored in the SSH keys section in the Metadata. "},{"url":"Web/UFW.html","title":"UFW","level":"1.12.1","keywords":[],"body":"UFW Enable UFW Before enabling UFW, ensure that OpenSSH is allowed to prevent being locked out of your server: sudo ufw allow OpenSSH Now you can safely enable UFW: sudo ufw enable Check UFW Status sudo ufw status Set Default Policies Set the default policies to deny incoming and allow outgoing traffic: sudo ufw default deny incoming sudo ufw default allow outgoing Allow Specific Services or Ports Allow HTTP and HTTPS: sudo ufw allow 'Nginx HTTP' sudo ufw allow 'Nginx HTTPS' Allow a specific port, such as SSH: sudo ufw allow 22 Allow a specific IP to connect to a certain port: sudo ufw allow from 192.168.1.0/24 to any port 3306 Deny Specific Ports Directly deny a specific port: sudo ufw deny 23 Delete Rules Remove a rule that allows a particular service, such as 'Nginx HTTP': sudo ufw delete allow 'Nginx HTTP' Reset UFW Reset UFW to the default state; all rules will be deleted: sudo ufw reset Disable UFW Disable UFW to allow all traffic: sudo ufw disable Additional Commands for Specific Scenarios Set Named Services: Some services register with UFW, such as 'Nginx Full', which can be used to allow both HTTP and HTTPS. sudo ufw allow 'Nginx Full' Check Status with Details: Verify the final status to ensure the settings are correct. sudo ufw status verbose Logging: Enable logging to monitor security issues. sudo ufw logging on These commands will help effectively manage your server's UFW firewall rules, ensuring security and proper service operation. "},{"url":"Web/Nginx.html","title":"Nginx","level":"1.12.2","keywords":[],"body":"Nginx Installation sudo apt install nginx If you have the ufw firewall running, you will need to allow connections to Nginx. You should enable the most restrictive profile that will still allow the traffic you want. Since you haven‚Äôt configured SSL for your server yet, for now you only need to allow HTTP traffic on port 80. You can enable this by typing: sudo ufw allow 'Nginx HTTP' You can verify the change by typing: sudo ufw status You should see HTTP traffic allowed in the displayed output: Output Status: active To Action From -- ------ ---- OpenSSH ALLOW Anywhere Nginx HTTP ALLOW Anywhere OpenSSH (v6) ALLOW Anywhere (v6) Nginx HTTP (v6) ALLOW Anywhere (v6) If need www-data permission sudo usermod -a -G www-data $USER sudo chown -R www-data:www-data /var/www/ sudo chmod -R g+w /var/www/ Check Nginx config file syntax nginx -t -c /etc/nginx/nginx.conf Anti-leech server { ... location ~* .(gif|jpe?g|png|webp|bmp)$ { expires 30d; access_log /dev/null; valid_referers none blocked server_names *.example.com ~.facebook. ~.google. ~.baidu. ~.bing. ~.yahoo.; if ($invalid_referer) { # Rewrite to the site advertising map rewrite ^/ /refer.png redirect; # return 403; } } # Avoid redirect loop location = /refer.png {} } Add a new site with PHP support Create a new directory for the site sudo mkdir /var/www/example.com Change the owner of the directory to the current user sudo chown -R $USER:$USER /var/www/example.com Create a new configuration file for the site sudo vim /etc/nginx/sites-available/example.com Add the following content to the file server { listen 80; listen [::]:80; #Increase file upload maximum size client_max_body_size 256M; # Site root directory root /var/www/example.com; index index.php index.html index.htm; # Domain name server_name example.com www.example.com; location / { try_files $uri $uri/ =404; } location ~ \\.php$ { include snippets/fastcgi-php.conf; fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; } } Create a symbolic link to the configuration file sudo ln -s /etc/nginx/sites-available/example.com /etc/nginx/sites-enabled/ Test the configuration file sudo nginx -t Reload the Nginx service sudo systemctl reload nginx If you want to maintain the directory of the site easily, you can create a symbolic link to the directory sudo ln -s /var/www/example.com /home/$USER/example.com Allowing HTTPS Through the Firewall If you have the ufw firewall enabled, as recommended by the prerequisite guides, you‚Äôll need to adjust the settings to allow for HTTPS traffic. Luckily, Nginx registers a few profiles with ufw upon installation. You can see the current setting by typing: sudo ufw status It will probably look like this, meaning that only HTTP traffic is allowed to the web server: Output Status: active To Action From -- ------ ---- OpenSSH ALLOW Anywhere Nginx HTTP ALLOW Anywhere OpenSSH (v6) ALLOW Anywhere (v6) Nginx HTTP (v6) ALLOW Anywhere (v6) To additionally let in HTTPS traffic, allow the Nginx Full profile and delete the redundant Nginx HTTP profile allowance: sudo ufw allow 'Nginx Full' sudo ufw delete allow 'Nginx HTTP' Your status should now look like this: sudo ufw status Output Status: active To Action From -- ------ ---- OpenSSH ALLOW Anywhere Nginx Full ALLOW Anywhere OpenSSH (v6) ALLOW Anywhere (v6) Nginx Full (v6) ALLOW Anywhere (v6) Set LetsEncrypt: Install certbot sudo apt install certbot python3-certbot-nginx Obtaining an SSL Certificate sudo certbot --nginx -d example.com -d www.example.com Verifying Certbot Auto-Renewal Let‚Äôs Encrypt‚Äôs certificates are only valid for ninety days. This is to encourage users to automate their certificate renewal process. The certbot package we installed takes care of this for us by adding a systemd timer that will run twice a day and automatically renew any certificate that‚Äôs within thirty days of expiration. You can query the status of the timer with systemctl: sudo systemctl status certbot.timer To test the renewal process, you can do a dry run with certbot: sudo certbot renew --dry-run If you see no errors, you‚Äôre all set. When necessary, Certbot will renew your certificates and reload Nginx to pick up the changes. If the automated renewal process ever fails, Let‚Äôs Encrypt will send a message to the email you specified, warning you when your certificate is about to expire. Nginx Reverse Proxy with Cloudflare Normal Secure server { listen 80; listen [::]:80; root /var/www/example.com; index index.php index.html index.htm; server_name example.com; location / { proxy_pass http://127.0.0.1:8010; proxy_http_version 1.1; proxy_set_header Host $host; } } Full(Strict) Secure server { listen 80; listen [::]:80; listen 443 ssl http2; listen [::]:443 ssl http2; #root /var/www/example.com; #index index.php index.html index.htm; server_name example.com; ssl_certificate /etc/ssl/certs/cf_example.com.pem; ssl_certificate_key /etc/ssl/private/cf_example.com-key.pem; ssl_client_certificate /etc/ssl/certs/origin-pull-ca.pem; ssl_verify_client on; location / { proxy_pass http://127.0.0.1:8010; proxy_http_version 1.1; proxy_set_header Host $host; } } CDN server { listen 80; server_name cdn.domain.org; root /usr/share/nginx/cdn; location / { proxy_pass http://domain.org; proxy_set_header Host $http_host; proxy_set_header True-Client-IP $remote_addr; } location ~* \\.(jpg|png|gif|jpeg|webp|css|mp3|wav|swf|mov|doc|pdf|xls|ppt|docx|pptx|xlsx)$ { expires max; proxy_set_header X-Real-IP $remote_addr; proxy_pass http://domain.org; proxy_ignore_headers X-Accel-Expires Expires Cache-Control; proxy_store /usr/share/nginx/cdn$uri; proxy_store_access user:rw group:rw all:r; proxy_read_timeout 60s; proxy_connect_timeout 60s; proxy_send_timeout 60s; } location ~ /\\.ht { deny all; } location ~ ~\\$ { deny all; } # An appropriate PHP handler should be set up here # location ~ \\.php$ { deny all; } } "},{"url":"Web/LetsEncrypt.html","title":"Let's Encrypt","level":"1.12.3","keywords":[],"body":"Let's Encrypt Remove (revoke) a domain in Let‚Äôs Encrypt To remove a Let‚Äôs Encrypt certificate from a domain no longer served from server.In this example, I use the www.mydomain.com domain. I will remove it in 3 steps: Backup Revoke the certificate Delete all files relating to the certificate # ‚Äì Indicates that the command that follows must be executed with root permissions directly with the root user or with the sudo command.$ ‚Äì Indicates that the following command can be executed by a normal user without administrative privileges. Backup First, make a backup sudo cp /etc/letsencrypt/ /etc/letsencrypt.backup -r Revoke Then revoke the cert sudo certbot revoke --cert-path /etc/letsencrypt/archive/www.mydomain.com/cert1.pem # Saving debug log to /var/log/letsencrypt/letsencrypt.log # Starting new HTTPS connection (1): acme-v01.api.letsencrypt.org Delete the files Finally, delete all files relating to certificate www.mydomain.com sudo certbot delete # Saving debug log to /var/log/letsencrypt/letsencrypt.log Which certificate would you like to delete? ------------------------------------------------------------------------------- 1: www.domain1.com 2: www.domain2.com 3: www.mydomain.com ------------------------------------------------------------------------------- Select the appropriate number [1-6] then [enter] (press 'c' to cancel): 3 ------------------------------------------------------------------------------- Deleted all files relating to certificate www.mydomain.com. ------------------------------------------------------------------------------- "},{"url":"qBittorrent/Setup.html","title":"qBittorrent Setup","level":"1.13.1","keywords":[],"body":"qBittorrent Setup Install qBittorrent on Ubuntu Import qBittorrent PPA on Ubuntu In this section, we‚Äôll import the qBittorrent Launchpad PPA (Personal Package Archive) to access the qBittorrent packages for Ubuntu. Step 1: Update Ubuntu Before qBittorrent Installation sudo apt update && sudo apt upgrade -y Step 2: Install Initial Packages For qBittorrent on Ubuntu sudo apt install dirmngr ca-certificates software-properties-common apt-transport-https Step 3: Import qBittorrent PPA on UbuntuOption 1: Import qBittorrent stable PPA: sudo add-apt-repository ppa:qbittorrent-team/qbittorrent-stable -y Option 2: Import qBittorrent unstable PPA: sudo add-apt-repository ppa:qbittorrent-team/qbittorrent-unstable -y Step 4: Update Packages List After qBittorrent PPA Import on Ubuntu sudo apt update Install qBittorrent Desktop Client In this section, we‚Äôll install the qBittorrent desktop client on your Ubuntu system and launch it for the first time. Step 1: Install qBittorrent Desktop Client via APT command sudo apt install qbittorrent Step 2: Launch qBittorrent Desktop Client qbittorrent Install qBittorrent-nox Web-UI (Ubuntu Server) qBittorrent-nox allows you to install qBittorrent on a headless Ubuntu server or a remotely accessed desktop. With the WebUI interface, you can efficiently manage qBittorrent using your favorite browser. Step 1: Install qBittorrent-nox via APT command sudo apt install qbittorrent-nox qBittorrent-nox is designed for headless clients and is accessible via a Web interface at the default localhost location: http://localhost:8080. The Web UI access is secured by default.The default username is admin, and the default password is adminadmin. Step 2: Create a User and Group for qbittorrent-nox on Ubuntu sudo adduser qbtuser Step 3: Disable qbtuser account SSH login (optional)You may also want to disable login for the account (from SSH) for security reasons. The account will still be usable locally: sudo usermod -s /usr/sbin/nologin qbtuser This can be reversed if necessary with the command: sudo usermod -s /bin/bash qbtuser Step 4: Add Username to qBittorrent Group on Ubuntu sudo adduser $USER qbtuser Step 5: Configure a Systemd Service File for qbittorrent-nox on UbuntuModify a systemd service file for qbittorrent-nox: sudo vim /usr/lib/systemd/system/qbittorrent-nox@.service Copy and paste the following lines into the file: [Unit] Description=qBittorrent-nox service for user %I Documentation=man:qbittorrent-nox(1) Wants=network-online.target After=local-fs.target network-online.target nss-lookup.target [Service] Type=forking PrivateTmp=false User=%i Group=%i UMask=007 ExecStart=/usr/bin/qbittorrent-nox -d --webui-port=8080 TimeoutStopSec=1800 Restart=on-failure [Install] WantedBy=multi-user.target Step 6: Reload the Systemd Daemon for qbittorrent-nox on Ubuntu sudo systemctl daemon-reload Step 7: Add default config file for qbittorrent-nox on Ubuntu WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/qBittorrent/setup-qbttorrent-conf.sh && chmod +x setup-qbttorrent-conf.sh && bash setup-qbttorrent-conf.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/qBittorrent/setup-qbttorrent-conf.sh && chmod +x setup-qbttorrent-conf.sh && bash setup-qbttorrent-conf.sh Step 8: Start and Enable qbittorrent-nox Service on Ubuntu sudo systemctl start qbittorrent-nox@qbtuser sudo systemctl enable qbittorrent-nox@qbtuser Before proceeding, check the status to ensure everything is working correctly: sudo systemctl status qbittorrent-nox@qbtuser "},{"url":"qBittorrent/Uninstall.html","title":"qBittorrent Uninstall","level":"1.13.2","keywords":[],"body":"qBittorrent Uninstall Uninstall qBittorrent on Ubuntu Remove qBittorrent or qBittorrent-nox From Ubuntu If you wish to uninstall qBittorrent, follow this simple process. First, remove the custom PPA if you installed it according to the previous tutorial.To remove the PPA you imported, use the following commands: Remove qBittorrent stable PPA: sudo add-apt-repository --remove ppa:qbittorrent-team/qbittorrent-stable -y Remove qBittorrent unstable PPA: sudo add-apt-repository --remove ppa:qbittorrent-team/qbittorrent-unstable -y Now, remove the qBittorrent package: sudo apt remove qbittorrent qbittorrent-nox -y Finally, remove the residual configuration files: sudo apt purge qbittorrent qbittorrent-nox -y Remove qBittorrent Data and Configuration Files To remove the qBittorrent data and configuration files, use the following commands: rm -rf ~/.config/qBittorrent rm -rf ~/.local/share/data/qBittorrent Remove qBittorrent User and Group To remove the qBittorrent user and group, use the following commands: sudo deluser --remove-home qbtuser Remove qBittorrent Log Files To remove the qBittorrent log files, use the following commands: sudo rm -rf /var/log/qbittorrent Remove qBittorrent Systemd Service To remove the qBittorrent systemd service, use the following commands: sudo systemctl stop qbittorrent-nox sudo systemctl disable qbittorrent-nox sudo rm /etc/systemd/system/qbittorrent-nox.service sudo systemctl daemon-reload sudo systemctl reset-failed Remove qBittorrent-nox Web-UI To remove the qBittorrent-nox Web-UI, use the following commands: sudo rm -rf /usr/share/qbittorrent-nox Remove qBittorrent-nox Configuration Files To remove the qBittorrent-nox configuration files, use the following commands: sudo rm -rf /etc/qBittorrent Remove qBittorrent-nox User and Group To remove the qBittorrent-nox user and group, use the following commands: sudo deluser --remove-home qbtuser Remove qBittorrent-nox Log Files To remove the qBittorrent-nox log files, use the following commands: sudo rm -rf /var/log/qbittorrent-nox Remove qBittorrent-nox Systemd Service To remove the qBittorrent-nox systemd service, use the following commands: sudo systemctl stop qbittorrent-nox sudo systemctl disable qbittorrent-nox sudo rm /etc/systemd/system/qbittorrent-nox@.service sudo systemctl daemon-reload sudo systemctl reset-failed "},{"url":"System/","title":"System","level":"1.14","keywords":[],"body":"System swap.sh Description: Swap Easy Setup Tool Support OS: Debian 6+ / Ubuntu 14+ Installation: WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/System/swap.sh && chmod +x swap.sh && bash swap.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/System/swap.sh && chmod +x swap.sh && bash swap.sh "},{"url":"Proxy/SSR.html","title":"SSR","level":"1.15.1","keywords":[],"body":"Proxy ssr.sh Description: ShadowsocksR Easy Setup Tool Support OS: CentOS 6+ / Debian 6+ / Ubuntu 14+ Feature Support setting limit of user speed Support setting limit of the number of port devices Support displaying current connected IP Support displaying SS / SSR connection info & QRcode Support switching single-port / multi-port mode Support easy installation of BBR Note If you want to get location of IP, you should get the token from IP-Info Installation WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Proxy/SSR/ssr.sh && chmod +x ssr.sh && bash ssr.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Proxy/SSR/ssr.sh && chmod +x ssr.sh && bash ssr.sh Firewall Firewall-cmd command for SSR firewall-cmd --permanent --add-port=7382/tcp firewall-cmd --permanent --add-port=7382/udp firewall-cmd --reload "},{"url":"Proxy/VPN.html","title":"VPN","level":"1.15.2","keywords":[],"body":"Proxy vpn.sh Description: IPsec VPN Easy Setup Tool Support OS: Debian 9+ / Ubuntu 14+ Installation: WGET wget -N --no-cache --no-check-certificate https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Proxy/VPN/vpn.sh && chmod +x vpn.sh && bash vpn.sh CURL curl -H 'Cache-Control: no-cache' -O https://raw.githubusercontent.com/carry0987/Linux-Script/master/book_source/Proxy/VPN/vpn.sh && chmod +x vpn.sh && bash vpn.sh "},{"url":"Git/","title":"Git","level":"1.16","keywords":[],"body":"Git Find credentials all over the place docker run --platform linux/arm64 -it -v \"$PWD:/pwd\" trufflesecurity/trufflehog:latest github --repo [URL_OF_REPO] --token=[GITHUB_PERSONAL_ACCESS_TOKEN] Git Case Sensitivity If you are facing issues with case sensitivity in git, you can disable it by running the following command. This will make git case sensitive. Note that MacOS is case insensitive by default.So, if you are using MacOS, you should run the following command to make git case sensitive. git config --global core.ignorecase false Git Reset Add the following function to ~/.bashrc or ~/.zshrc to reset the git commit. git_reset() { read 'commit_val?Enter the value for the number of commit which you want to reset >' if [[ -n $commit_val ]]; then git reset --hard HEAD~$commit_val git push --force fi } "},{"url":"Python/Conda.html","title":"Conda","level":"1.17.1","keywords":[],"body":"Conda Set default env to test activate.py is hardcoded to emit conda activate base\\n into your shell profile when you evaluate the shell hook produced by conda shell.zsh hook. You can suppress this hardcoded \"auto-activate base\" via: conda config --set auto_activate_base false Create test env: conda create -n test Then, in ~/.zshrc, ~/.bashrc or wherever you source your shell profile from, you can append the following (after the conda shell hook) to explicitly activate the environment of your choosing: conda activate test Then close and reopen the terminal, you will see the test env is activated by default. "},{"url":"Python/Jupyter.html","title":"Jupyter","level":"1.17.2","keywords":[],"body":"Jupyter Notebook Install Jupyter Notebook Install Jupyter Notebook: conda install notebook -y pip install numpy --upgrade pip install pandas --upgrade pip install matplotlib --upgrade pip install scikit-learn --upgrade pip install scipy --upgrade pip install plotly --upgrade Generate config file for Jupyter Notebook: jupyter notebook --generate-config --notebook-dir=\"/Users/$USER/Documents/Project/AI/Jupyter\" --no-browser MacOS Install the following packages for Apple Silicon core: conda install -c apple tensorflow-deps --upgrade pip install tensorflow --upgrade pip install tensorflow-metal --upgrade "},{"url":"MacOS/zshrc.html","title":"zshrc","level":"1.18.1","keywords":[],"body":"zshrc source ~/.zprofile PROMPT='%F{033}%n%F{reset}@%F{green}%m:%F{yellow}%B%~%b%F{reset}$ %F{reset}' #export JAVA_HOME=$(/usr/libexec/java_home -v 1.11) #export JAVA_HOME=/Library/Java/JavaVirtualMachines/zulu-11.jdk/Contents/Home export GPG_TTY=$(tty) gpgconf --launch gpg-agent # Color for ls command export CLICOLOR=1 export LSCOLORS=ExFxBxDxCxegedabagacad # History config #use a history file in here HISTFILE=${ZDOTDIR:-$HOME}/.zsh_history # make it huge, really huge. SAVEHIST=1000000 HISTSIZE=1000000 # there is for sure still some redundancy, but ... # setopt BANG_HIST # Treat the '!' character specially during expansion. # setopt EXTENDED_HISTORY # Write the history file in the \":start:elapsed;command\" format. setopt INC_APPEND_HISTORY # Write to the history file immediately, not when the shell exits. #setopt SHARE_HISTORY # Share history between all sessions. setopt HIST_EXPIRE_DUPS_FIRST # Expire duplicate entries first when trimming history. setopt HIST_IGNORE_DUPS # Don't record an entry that was just recorded again. #setopt HIST_IGNORE_ALL_DUPS # Delete old recorded entry if new entry is a duplicate. setopt HIST_FIND_NO_DUPS # Do not display a line previously found. setopt HIST_IGNORE_SPACE # Don't record an entry starting with a space. #setopt HIST_SAVE_NO_DUPS # Don't write duplicate entries in the history file. setopt HIST_REDUCE_BLANKS # Remove superfluous blanks before recording entry. #setopt HIST_VERIFY # Don't execute immediately upon history expansion. #setopt HIST_BEEP # Beep when accessing nonexistent history. alias history=\"history 0\" "},{"url":"MacOS/screenrc.html","title":"screenrc","level":"1.18.2","keywords":[],"body":"screenrc startup_message off term screen-256color "},{"url":"MacOS/tftp.html","title":"tftp","level":"1.18.3","keywords":[],"body":"TFTP Start TFTP server Activate the tftp server on your Mac: To change the properties, edit the file /System/Library/LaunchDaemons/tftp.plist The default directory is /private/tftpboot. Make this directory accessible for everybody. sudo chmod 777 /private/tftpboot sudo launchctl start com.apple.tftpd And start it with sudo launchctl load -F /System/Library/LaunchDaemons/tftp.plist Stop TFTP server If you want to stop the daemon, do sudo launchctl unload /System/Library/LaunchDaemons/tftp.plist sudo launchctl stop com.apple.tftpd Change back the permissions of the directory to /private/tftpboot sudo chmod 755 /private/tftpboot "},{"url":"MacOS/Config.html","title":"Config","level":"1.18.4","keywords":[],"body":"MacOS Screenshot Save PathDefault defaults write com.apple.screencapture location ~/Desktop Change defaults write com.apple.screencapture location ~/Documents/Screenshot Change File Name defaults write com.apple.screencapture name Screenshot-{NAME} Change File Type defaults write com.apple.screencapture type png Reload SystemUI killall SystemUIServer "}]